<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Tanguy Ophoff</title> <meta name="author" content="Tanguy Ophoff"> <meta name="description" content="Tanguy Ophoff's portfolio detailing the projects he worked on and the publications he made during his PhD. "> <meta name="keywords" content="portfolio, tanguy, ophoff, computer vision, engineer, phd, research, deep learning, object detection"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?1d6c5b8a647fb793d78002ad7326eb76"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ophoff.dev/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tanguy </span>Ophoff</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1></header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div id="ophoff_phd" class="col-sm-10"> <div class="title">Aerial Object Detection</div> <div class="author"> <em>Tanguy Ophoff</em>, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lirias.kuleuven.be/retrieve/717234" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Nowadays, most computer vision problems are solved using artificial intelligence. These techniques outperform traditional computer vision algorithms in most scenarios and even allow the use of computer vision for a variety of challenging new fields. One of these fields is remote sensing. Using artificial intelligence we are able to automatically extract complex metadata, which aids in the decision making process of governments, industries, etc. Nevertheless, several key challenges remain to be solved in order to successfully deploy these algorithms:</p> <ol> <li>One of the main challenges in this field is to cope with the huge amount of data. Indeed, aerial orthomosaics are often in the order of 10⁹ square pixels large. Detecting objects, which can be as small as a few hundred square pixels in size, quickly becomes extremely challenging.</li> <li>Privacy is a big social issue with remote sensing data. A lot of aerial images indeed capture data across huge regions, disregarding private areas or people who might be visible in the data. One possible solution is to process the images on-board on the sensor devices themselves. However, running artificial neural networks on constrained devices remains a major challenge.</li> <li>The majority of computer vision algorithms work on traditional red-green-blue image data. However, a lot of remote sensing sensors offer additional types of data, giving opportunities to improve our algorithms. We still need to find a solution to optimally use these new types of data and to integrate them with the traditional image data. During this PhD we worked on three different object detection use cases, while finding an optimal solution for the aforementioned challenges. Firstly, we developed a pipeline to run object detection networks on remote sensing data. Our initial pipeline processed the orthomosaic with a sliding window, adding overlap between the different image patches. While this proved it is possible to adapt artificial intelligence to remote sensing use cases, we also improved the results significantly by implementing a series of scene-specific pre- and post-processing steps. Secondly, we researched the added value of sensor fusion. More specifically, we developed a technique to merge different types of data in a neural network and applied it to object detection on red-green-blue and depth data. We tested our technique on a variety of different datasets, demonstrating the benefit of fusing this data for both natural and remote sensing images. Thirdly, we implemented a series of techniques in order to reduce the computational complexity of our algorithms, with the goal of running them in real-time on embedded devices. By combining mobile convolutions, pruning and quantisation techniques, we were able to reduce the complexity of a neural network significantly, without sacrificing accuracy. To summarize, we developed a variety of techniques which enable object detection networks to run on remote sensing data, clearly demonstrating its feasibility. We also showed that it is possible to further increase the accuracy of our models, when different types of data are available. Finally, we determined that many neural networks are oversized for most tasks, allowing to reduce the computational complexity without sacrificing accuracy.</li> </ol> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">ophoff_phd</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Aerial Object Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Goedem{\'e}, Toon and Van Beeck, Kristof}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2023-06-26}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://lirias.kuleuven.be/4085681}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="ophoff_plane" class="col-sm-10"> <div class="title">Improving Object Detection in VHR Aerial Orthomosaics</div> <div class="author"> <em>Tanguy Ophoff</em>, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Computer Vision Workshops (ECCV Workshops)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://lirias.kuleuven.be/retrieve/701654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper we investigate how to improve object detection on very high resolution orthomosaics. For this, we present a new detection model ResnetYolo, with a Resnet50 backbone and selectable detection heads. Furthermore, we propose two novel techniques to post-process the object detection results: a neighbour based patch NMS algorithm and an IoA based filtering technique. Finally, we fuse color and depth data in order to further increase the results of our deep learning model. We test these improvements on two distinct, challenging use cases: solar panel and swimming pool detection. The images are very high resolution color and elevation orthomosaics, taken from plane photography. Our final models reach an average precision of 78.5% and 44.4% respectively, outperforming the baseline models by over 15% AP.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ophoff_plane</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Object Detection in VHR Aerial Orthomosaics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Van Beeck, Kristof and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Computer Vision Workshops (ECCV Workshops)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-25066-8}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div id="ophoff_speed" class="col-sm-10"> <div class="title">Investigating the Potential of Network Optimization for a Constrained Object Detection Problem</div> <div class="author"> <em>Tanguy Ophoff</em>, Cédric Gullentops, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>Journal of Imaging</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/2313-433X/7/4/64" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://lirias.kuleuven.be/retrieve/620965" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Object detection models are usually trained and evaluated on highly complicated, challenging academic datasets, which results in deep networks requiring lots of computations. However, a lot of operational use-cases consist of more constrained situations: they have a limited number of classes to be detected, less intra-class variance, less lighting and background variance, constrained or even fixed camera viewpoints, etc. In these cases, we hypothesize that smaller networks could be used without deteriorating the accuracy. However, there are multiple reasons why this does not happen in practice. Firstly, overparameterized networks tend to learn better, and secondly, transfer learning is usually used to reduce the necessary amount of training data. In this paper, we investigate how much we can reduce the computational complexity of a standard object detection network in such constrained object detection problems. As a case study, we focus on a well-known single-shot object detector, YoloV2, and combine three different techniques to reduce the computational complexity of the model without reducing its accuracy on our target dataset. To investigate the influence of the problem complexity, we compare two datasets: a prototypical academic (Pascal VOC) and a real-life operational (LWIR person detection) dataset. The three optimization steps we exploited are: swapping all the convolutions for depth-wise separable convolutions, perform pruning and use weight quantization. The results of our case study indeed substantiate our hypothesis that the more constrained a problem is, the more the network can be optimized. On the constrained operational dataset, combining these optimization techniques allowed us to reduce the computational complexity with a factor of 349, as compared to only a factor 9.8 on the academic dataset. When running a benchmark on an Nvidia Jetson AGX Xavier, our fastest model runs more than 15 times faster than the original YoloV2 model, whilst increasing the accuracy by 5% Average Precision (AP).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ophoff_speed</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Gullentops, C{\'e}dric and Van Beeck, Kristof and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Investigating the Potential of Network Optimization for a Constrained Object Detection Problem}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Imaging}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">article-number</span> <span class="p">=</span> <span class="s">{64}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2313-433X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/jimaging7040064}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2313-433X/7/4/64}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="ophoff_erti" class="col-sm-10"> <div class="title">Real-Time Embedded Computer Vision on UAVs: UAVision2020 Workshop Summary</div> <div class="author"> <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, Maarten Vandersteegen, <em>Tanguy Ophoff</em>, Tinne Tuytelaars, Davide Scaramuzza, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>In Computer Vision – ECCV 2020 Workshops</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In this paper we present an overview of the contributed work presented at the UAVision2020 (International workshop on Computer Vision for UAVs) ECCV workshop. Note that during ECCV2020 this workshop was merged with the VisDrone2020 workshop. This paper only summarizes the results of the regular paper track and the ERTI challenge. The workshop focused on real-time image processing on-board of Unmanned Aerial Vehicles (UAVs). For such applications the computational complexity of state-of-the-art computer vision algorithms often conflicts with the need for real-time operation and the extreme resource limitations of the hardware. Apart from a summary of the accepted workshop papers and an overview of the challenge, this work also aims to identify common challenges and concerns which were addressed by multiple authors during the workshop, and their proposed solutions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ophoff_erti</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Van Beeck, Kristof and Vandersteegen, Maarten and Ophoff, Tanguy and Tuytelaars, Tinne and Scaramuzza, Davide and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-Time Embedded Computer Vision on UAVs: UAVision2020 Workshop Summary}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2021-01-03}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{978-3-030-66823-5}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Vision – ECCV 2020 Workshops}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{665 - 674}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div id="ophoff_satellite" class="col-sm-10"> <div class="title">Vehicle and Vessel Detection on Satellite Imagery: A Comparative Study on Single-Shot Detectors</div> <div class="author"> <em>Tanguy Ophoff</em>, Steven Puttemans, Vasileios Kalogirou, Jean-Philippe Robin, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>Remote Sensing</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/2072-4292/12/7/1217" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://lirias.kuleuven.be/retrieve/573243" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we investigate the feasibility of automatic small object detection, such as vehicles and vessels, in satellite imagery with a spatial resolution between 0.3 and 0.5 m. The main challenges of this task are the small objects, as well as the spread in object sizes, with objects ranging from 5 to a few hundred pixels in length. We first annotated 1500 km2, making sure to have equal amounts of land and water data. On top of this dataset we trained and evaluated four different single-shot object detection networks: YOLOV2, YOLOV3, D-YOLO and YOLT, adjusting the many hyperparameters to achieve maximal accuracy. We performed various experiments to better understand the performance and differences between the models. The best performing model, D-YOLO, reached an average precision of 60% for vehicles and 66% for vessels and can process an image of around 1 Gpx in 14 s. We conclude that these models, if properly tuned, can thus indeed be used to help speed up the workflows of satellite data analysts and to create even bigger datasets, making it possible to train even better models in the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ophoff_satellite</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Puttemans, Steven and Kalogirou, Vasileios and Robin, Jean-Philippe and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vehicle and Vessel Detection on Satellite Imagery: A Comparative Study on Single-Shot Detectors}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">article-number</span> <span class="p">=</span> <span class="s">{1217}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2072-4292}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/rs12071217}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2072-4292/12/7/1217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div id="ophoff_rgbd02" class="col-sm-10"> <div class="title">Exploring RGB+Depth Fusion for Real-Time Object Detection</div> <div class="author"> <em>Tanguy Ophoff</em>, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>Sensors</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/1424-8220/19/4/866" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://lirias.kuleuven.be/retrieve/537525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we investigate whether fusing depth information on top of normal RGB data for camera-based object detection can help to increase the performance of current state-of-the-art single-shot detection networks. Indeed, depth sensing is easily acquired using depth cameras such as a Kinect or stereo setups. We investigate the optimal manner to perform this sensor fusion with a special focus on lightweight single-pass convolutional neural network (CNN) architectures, enabling real-time processing on limited hardware. For this, we implement a network architecture allowing us to parameterize at which network layer both information sources are fused together. We performed exhaustive experiments to determine the optimal fusion point in the network, from which we can conclude that fusing towards the mid to late layers provides the best results. Our best fusion models significantly outperform the baseline RGB network in both accuracy and localization of the detections.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ophoff_rgbd02</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Van Beeck, Kristof and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring RGB+Depth Fusion for Real-Time Object Detection}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{19}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">article-number</span> <span class="p">=</span> <span class="s">{866}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1424-8220}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/s19040866}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MDPI}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/1424-8220/19/4/866}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div id="ophoff_rgbd01" class="col-sm-10"> <div class="title">Improving Real-Time Pedestrian Detectors with RGB+Depth Fusion</div> <div class="author"> <em>Tanguy Ophoff</em>, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> <em>In 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8639110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://lirias.kuleuven.be/retrieve/523031" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper we investigate the benefit of using depth information on top of normal RGB for camera-based pedestrian detection. Indeed, depth sensing is easily acquired using depth cameras such as a Kinect or stereo setups. We investigate the best way to perform this sensor fusion with a special focus on lightweight single-pass CNN architectures, enabling real-time processing on limited hardware. We implement different network architectures, each fusing depth at different layers of our network. Our experiments show that midway fusion performs the best, outperforming a regular RGB detector substantially in accuracy. Moreover, we prove that our fusion network is better at detecting individuals in a crowd, by demonstrating that it has both a better localization of pedestrians and is better at handling occluded persons. The resulting network is computationally efficient and achieves real-time performance on both desktop and embedded GPUs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ophoff_rgbd01</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Real-Time Pedestrian Detectors with RGB+Depth Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ophoff, Tanguy and Van Beeck, Kristof and Goedem{\'e}, Toon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/AVSS.2018.8639110}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8639110}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div id="ophoff_relabeled_epfl" class="col-sm-10"> <div class="title">Relabeled EPFL RGB-D Pedestrian Dataset</div> <div class="author"> <em>Tanguy Ophoff</em>, <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00062511" rel="external nofollow noopener" target="_blank">Kristof Van Beeck</a>, and <a href="https://iiw.kuleuven.be/onderzoek/eavise/people/00041334" rel="external nofollow noopener" target="_blank">Toon Goedemé</a> </div> <div class="periodical"> 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://www.eavise.be/dataset-rgbd-pedestrian-relabeled" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The EPFL RGB-D Pedestrian dataset consists of over 5000 RGB + Depth images acquired from an RGB camera and Kinect V2 sensor setup. However, the annotations for this dataset do not include persons that are highly occluded. This is why we decided to relabel the entire dataset manually, providing bounding boxes for every person in the image - independent of their occlusion level - and instead adding an occlusion percentage value. This relabeled dataset was then used to assess the added value of fusing RGB + depth in a single-pass detection network.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom"> <div class="container"> © Copyright 2023 Tanguy Ophoff. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: November 06, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>